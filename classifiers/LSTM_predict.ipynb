{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "# Load the model\n",
    "model = load_model('../classifiers/trained_models/LSTM_high_acc9437_9140_9157.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_posTag_features(text_posTag):\n",
    "    adjective_tags = ['JJ', 'JJ$', 'JJ+JJ', 'JJR', 'JJR+CS', 'JJS', 'JJT', ]\n",
    "    noun_tags = ['NN', 'NN$', 'NN+BEZ', 'NN+HVD', 'NN+HVZ', 'NN+IN', 'NN+MD', 'NN+NN', 'NNS', 'NNS$', 'NNS+MD', 'NP', 'NP$', 'NP+BEZ', 'NP+HVZ', 'NP+MD', 'NPS', 'NPS$', 'NR', 'NR$', 'NR+MD', 'NRS']\n",
    "    pronoun_tags = ['PN', 'PN$', 'PN+BEZ', 'PN+HVD', 'PN+HVZ', 'PN+MD', 'PP$$', 'PPL', 'PPLS', 'PPO', 'PPS', 'PPS+BEZ', 'PPS+HVD', 'PPS+HVZ', 'PPS+MD', 'PPSS', 'PPSS+BEM', 'PPSS+BER', 'PPSS+BEZ', 'PPSS+BEZ*', 'PPSS+HV', 'PPSS+HVD', 'PPSS+MD', 'PPSS+VB']\n",
    "    article_tags = ['AT']\n",
    "    conjunction_tags = ['CC', 'CS']\n",
    "    numeral_tags = ['CD', 'CD$', 'OD']\n",
    "    preposition_tags = ['IN', 'IN+IN', 'IN+PPO']\n",
    "    qualifier_tags = ['QL', 'QLP']\n",
    "    adverb_tags = ['RB', 'RB$', 'RB+BEZ', 'RB+CS', 'RBR', 'RBR+CS', 'RBT', 'RN', 'RP', 'RP+IN']\n",
    "    foreign_word = 'FW'\n",
    "    w_classification_tags = ['WDT', 'WDT+BER', 'WDT+BER+PP', 'WDT+BEZ', 'WDT+DO+PPS', 'WDT+DOD', 'WDT+HVZ', 'WP$', 'WPO', 'WPS', 'WPS+BEZ', 'WPS+HVD', 'WPS+HVZ', 'WPS+MD', 'WQL', 'WRB','WRB+BER', 'WRB+BEZ', 'WRB+DO', 'WRB+DOD', 'WRB+DOD*', 'WRB+DOZ', 'WRB+IN', 'WRB+MD']\n",
    "    modal_tags = ['MD', 'MD*', 'MD+HV', 'MD+PPSS', 'MD+TO']\n",
    "    a_determiner_tags = ['ABL', 'ABN', 'ABX', 'AP', 'AP$', 'AP+AP']\n",
    "    determiner_tags = ['DT', 'DT$', 'DT+BEZ', 'DT+MD', 'DTI', 'DTS', 'DTX']\n",
    "    verb_tags = ['BE', 'BED', 'BED*', 'BEDZ', 'BEDZ*', 'BEG', 'BEM', 'BEM*', 'BEN', 'BER', 'BER*', 'BEZ', 'BEZ*', 'DO', 'DO*', 'DO+PPSS', 'DOD', 'DOD*', 'DOZ', 'DOZ*', 'HV','HV*', 'HV+TO', 'HVD', 'HVD*', 'HVG', 'HVN', 'HVZ', 'HVZ*', 'VB', 'VB+AT', 'VB+IN', 'VB+JJ', 'VB+PPO', 'VB+RP', 'VB+TO', 'VB+VB', 'VBD', 'VBG', 'VBG+TO', 'VBN','VBN+TO', 'VBZ']\n",
    "\n",
    "    n_posTags = len(text_posTag)\n",
    "\n",
    "    adjectives = 0\n",
    "    nouns = 0\n",
    "    pronouns = 0\n",
    "    articles = 0\n",
    "    conjunctions = 0\n",
    "    numerals = 0\n",
    "    prepositions = 0\n",
    "    qualifiers = 0\n",
    "    adverbs = 0\n",
    "    foreign_words = 0\n",
    "    w_classifications = 0\n",
    "    modals = 0\n",
    "    a_determiners = 0\n",
    "    determiners = 0\n",
    "    verbs = 0\n",
    "    for tag in text_posTag:\n",
    "        if tag in adjective_tags:\n",
    "            adjectives += 1\n",
    "        elif tag in noun_tags:\n",
    "            nouns += 1\n",
    "        elif tag in pronoun_tags:\n",
    "            pronouns += 1\n",
    "        elif tag in article_tags:\n",
    "            articles += 1\n",
    "        elif tag in conjunction_tags:\n",
    "            conjunctions += 1\n",
    "        elif tag in numeral_tags:\n",
    "            numerals += 1\n",
    "        elif tag in preposition_tags:\n",
    "            prepositions += 1\n",
    "        elif tag in qualifier_tags:\n",
    "            qualifiers += 1\n",
    "        elif tag in adverb_tags:\n",
    "            adverbs += 1\n",
    "        elif foreign_word in tag:\n",
    "            foreign_words += 1\n",
    "        elif tag in w_classification_tags:\n",
    "            w_classifications += 1\n",
    "        elif tag in modal_tags:\n",
    "            modals += 1\n",
    "        elif tag in a_determiner_tags:\n",
    "            a_determiners += 1\n",
    "        elif tag in determiner_tags:\n",
    "            determiners += 1\n",
    "        elif tag in verb_tags:\n",
    "            verbs += 1\n",
    "\n",
    "    adjectives_ratio = adjectives/n_posTags\n",
    "    nouns_ratio = nouns/n_posTags\n",
    "    pronouns_ratio = pronouns/n_posTags\n",
    "    articles_ratio = articles/n_posTags\n",
    "    conjunctions_ratio = conjunctions/n_posTags\n",
    "    numerals_ratio = numerals/n_posTags\n",
    "    prepositions_ratio = prepositions/n_posTags\n",
    "    qualifiers_ratio = qualifiers/n_posTags\n",
    "    adverbs_ratio = adverbs/n_posTags\n",
    "    foreign_words_ratio = foreign_words/n_posTags\n",
    "    w_classifications_ratio = w_classifications/n_posTags\n",
    "    modals_ratio = modals/n_posTags\n",
    "    a_determiners_ratio = a_determiners/n_posTags\n",
    "    determiners_ratio = determiners/n_posTags\n",
    "    verbs_ratio = verbs/n_posTags\n",
    "\n",
    "    if adjectives_ratio > 1 or nouns_ratio > 1:\n",
    "        print(f'Len_posTags: {n_posTags} --> Adjectives: {adjectives}, --> Nouns: {nouns}')\n",
    "\n",
    "    return adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio\n",
    "\n",
    "\n",
    "def extract_stopwords_and_ponctuation_ratio(text):\n",
    "\n",
    "    stopwords_count = 0\n",
    "    pontucation_count = 0\n",
    "\n",
    "\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    n_tokens = len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in stopwords_list:\n",
    "            stopwords_count += 1\n",
    "        elif token in string.punctuation:\n",
    "            pontucation_count += 1\n",
    "\n",
    "    stopwords_ratio = stopwords_count/n_tokens\n",
    "    pontucation_ratio = pontucation_count/n_tokens\n",
    "\n",
    "    if stopwords_ratio > 1 or pontucation_ratio > 1:\n",
    "        print(f'Len_tokens: {n_tokens} --> Stopwords: {stopwords_count}, --> Ponctuation: {pontucation_count}')\n",
    "\n",
    "    return stopwords_ratio, pontucation_ratio\n",
    "\n",
    "\n",
    "def extract_average_token_quantity_per_sentence(text):\n",
    "    tokens_quantity = []\n",
    "    sentences =  nltk.sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        tokens_quantity.append(len(sentence_tokens))\n",
    "\n",
    "    average_tokens_quantity_per_sentence = sum(tokens_quantity)//len(tokens_quantity)\n",
    "\n",
    "    return average_tokens_quantity_per_sentence\n",
    "    \n",
    "    \n",
    "\n",
    "def extract_average_token_length_per_text(text):\n",
    "\n",
    "    tokens_length = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        tokens_length.append(len(token))\n",
    "\n",
    "    average_token_length = sum(tokens_length)//len(tokens_length)\n",
    "\n",
    "    return average_token_length\n",
    "\n",
    "\n",
    "def extract_sentence_quantity_per_text(text):\n",
    "    sentences_quantity =  len(nltk.sent_tokenize(text))\n",
    "\n",
    "    if sentences_quantity > 300:\n",
    "        print(f'To many sentences: {text}')\n",
    "\n",
    "    return sentences_quantity\n",
    "    \n",
    "\n",
    "\n",
    "def extract_features(text, text_posTag, label=None, dataset=None, min_word_size=1, max_word_size=43, min_n_sentences_per_text=3, max_n_sentences_per_text=116,  min_n_words_per_sentence=1, max_n_words_per_sentence=943 ):\n",
    "    \n",
    "    adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio = extract_posTag_features(text_posTag)\n",
    "\n",
    "    stopwords_ratio, ponctuation_ratio = extract_stopwords_and_ponctuation_ratio(text)\n",
    "\n",
    "    average_token_quantity_per_sentence = extract_average_token_quantity_per_sentence(text)\n",
    "\n",
    "    average_token_length_per_text = extract_average_token_length_per_text(text)\n",
    "\n",
    "    \n",
    "    return [adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio, stopwords_ratio, ponctuation_ratio, average_token_quantity_per_sentence, average_token_length_per_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRP$', 'NN', 'RB', '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def get_pos_tags_from_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tag_tuples = nltk.pos_tag(tokens, lang='eng')\n",
    "    pos_tags = [tag[1] for tag in pos_tag_tuples]\n",
    "    return pos_tags\n",
    "\n",
    "# Example usage with raw text\n",
    "raw_text = \"Your text here.\"\n",
    "text_posTags = get_pos_tags_from_text(raw_text)\n",
    "\n",
    "# Now you can use text_posTags with the extract_features function\n",
    "features = extract_features(raw_text, text_posTags)\n",
    "## TODO: ALSO NEED TO NORMALiZE OR DO WE?\n",
    "text_posTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Return a list of 1s (AI) and 0s (Human), Input a list of sentences to classify \"\"\"\n",
    "def make_predictions(input_texts):\n",
    "    # Parameters\n",
    "    max_length = 200\n",
    "    padding_type = 'post'\n",
    "\n",
    "    # Load the pre-trained tokenizer\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        trained_tokenizer = pickle.load(handle)\n",
    "\n",
    "    text_pos = [\" \".join(get_pos_tags_from_text(text)) for text in input_texts]\n",
    "\n",
    "    # Tokenize the new text\n",
    "    new_text_sequences = trained_tokenizer.texts_to_sequences(text_pos)\n",
    "\n",
    "    # Pad the tokenized new text\n",
    "    new_text_padded = pad_sequences(new_text_sequences, padding=padding_type, maxlen=max_length)\n",
    "\n",
    "    # Make predictions on the new text\n",
    "    predictions = model.predict(new_text_padded)\n",
    "\n",
    "    # Convert probabilities to binary predictions for binary classification\n",
    "    binary_predictions = [1 if prob >= 0.5 else 0 for prob in predictions]\n",
    "\n",
    "    return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 26ms/step\n",
      "14/14 [==============================] - 0s 27ms/step\n",
      "HUMAN CORRECT: 0.960919540229885, AI CORRECT: 0.9908045977011494\n",
      "TOTAL DATA: 435, 435\n"
     ]
    }
   ],
   "source": [
    "# This is the raw text\n",
    "file = open('../dataset/news_pickles/train_test_val_Human', 'rb')\n",
    "X_human_train, X_human_test, X_human_val = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../dataset/news_pickles/train_test_val_GPT', 'rb')\n",
    "X_gpt_train, X_human_test, X_human_val = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "human_predictions = make_predictions(X_human_train) # Should predict 0\n",
    "ai_predictions = make_predictions(X_gpt_train) # Should predict 1\n",
    "\n",
    "human_correct_percent = sum(1 for pred in human_predictions if pred == 0) / len(human_predictions)\n",
    "ai_correct_percent = sum(1 for pred in ai_predictions if pred == 1) / len(ai_predictions)\n",
    "\n",
    "print(f\"HUMAN CORRECT: {human_correct_percent}, AI CORRECT: {ai_correct_percent}\")\n",
    "print(f\"TOTAL DATA: {len(X_human_train)}, {len(X_gpt_train)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
