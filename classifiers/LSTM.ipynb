{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the raw text\n",
    "file = open('../dataset/news_pickles/train_test_val_Human', 'rb')\n",
    "X_human_train, X_human_test, X_human_val = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../dataset/news_pickles/train_test_val_Human_posTags', 'rb')\n",
    "X_human_train_posTags, X_human_test_posTags, X_human_val_posTags = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "Y_human_train = np.zeros(len(X_human_train_posTags))\n",
    "Y_human_test = np.zeros(len(X_human_test_posTags))\n",
    "Y_human_val = np.zeros(len(X_human_val_posTags))\n",
    "\n",
    "\n",
    "file = open('../dataset/news_pickles/train_test_val_GPT_posTags', 'rb')\n",
    "X_gpt_train_posTags, X_gpt_test_posTags, X_gpt_val_posTags = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "Y_gpt_train = np.ones(len(X_gpt_train_posTags))\n",
    "Y_gpt_test = np.ones(len(X_gpt_test_posTags))\n",
    "Y_gpt_val = np.ones(len(X_gpt_val_posTags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 435, 95, 95, 93, 93)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_human_train_posTags), len(Y_human_train), len(X_human_test_posTags), len(Y_human_test), len(X_human_val_posTags), len(Y_human_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_human_train_posTags, X_gpt_train_posTags))\n",
    "Y_train = np.hstack((Y_human_train, Y_gpt_train))\n",
    "\n",
    "X_test = np.hstack((X_human_test_posTags, X_gpt_test_posTags))\n",
    "Y_test = np.hstack((Y_human_test, Y_gpt_test))\n",
    "\n",
    "X_val = np.hstack((X_human_val_posTags, X_gpt_val_posTags))\n",
    "Y_val = np.hstack((Y_human_val, Y_gpt_val))\n",
    "\n",
    "\n",
    "full_dataset = np.hstack((X_train, X_test, X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [' '.join(data) for data in X_train]\n",
    "X_test = [' '.join(data) for data in X_test]\n",
    "X_val = [' '.join(data) for data in X_val]\n",
    "\n",
    "full_dataset = [' '.join(data) for data in full_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 55\n",
    "oov_tok = ''\n",
    "embedding_dim = 150\n",
    "max_length = 200\n",
    "\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "tokenizer.fit_on_texts(full_dataset)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "val_padded = pad_sequences(val_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Assuming 'tokenizer' is your trained Keras Tokenizer. SAVE THIS TO USE IN PREDICTION\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded, Y_train = shuffle(train_padded, Y_train)\n",
    "test_padded, Y_test = shuffle(test_padded, Y_test)\n",
    "val_padded, Y_val = shuffle(val_padded, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((870, 200), (186, 200))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded.shape, val_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 150)          8250      \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 128)               110080    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122491 (478.48 KB)\n",
      "Trainable params: 122491 (478.48 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callback = EarlyStopping(monitor='accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/27\n",
      "28/28 [==============================] - 10s 266ms/step - loss: 0.6083 - accuracy: 0.6816 - val_loss: 0.4387 - val_accuracy: 0.7742\n",
      "Epoch 2/27\n",
      "28/28 [==============================] - 8s 270ms/step - loss: 0.3019 - accuracy: 0.8851 - val_loss: 0.4233 - val_accuracy: 0.7688\n",
      "Epoch 3/27\n",
      "28/28 [==============================] - 7s 263ms/step - loss: 0.2927 - accuracy: 0.8816 - val_loss: 0.3216 - val_accuracy: 0.8656\n",
      "Epoch 4/27\n",
      "28/28 [==============================] - 10s 346ms/step - loss: 0.2311 - accuracy: 0.9057 - val_loss: 0.2614 - val_accuracy: 0.8817\n",
      "Epoch 5/27\n",
      "28/28 [==============================] - 11s 376ms/step - loss: 0.2300 - accuracy: 0.9230 - val_loss: 0.2594 - val_accuracy: 0.9032\n",
      "Epoch 6/27\n",
      "28/28 [==============================] - 11s 376ms/step - loss: 0.2449 - accuracy: 0.9069 - val_loss: 0.4075 - val_accuracy: 0.8333\n",
      "Epoch 7/27\n",
      "28/28 [==============================] - 11s 377ms/step - loss: 0.1947 - accuracy: 0.9230 - val_loss: 0.2281 - val_accuracy: 0.9247\n",
      "Epoch 8/27\n",
      "28/28 [==============================] - 11s 375ms/step - loss: 0.1856 - accuracy: 0.9322 - val_loss: 0.3332 - val_accuracy: 0.8817\n",
      "Epoch 9/27\n",
      "28/28 [==============================] - 11s 380ms/step - loss: 0.2023 - accuracy: 0.9264 - val_loss: 0.2160 - val_accuracy: 0.9140\n",
      "Epoch 10/27\n",
      "28/28 [==============================] - 11s 380ms/step - loss: 0.1859 - accuracy: 0.9276 - val_loss: 0.2193 - val_accuracy: 0.9301\n",
      "Epoch 11/27\n",
      "28/28 [==============================] - 11s 379ms/step - loss: 0.1752 - accuracy: 0.9391 - val_loss: 0.2047 - val_accuracy: 0.9032\n",
      "Epoch 12/27\n",
      "28/28 [==============================] - 11s 383ms/step - loss: 0.2437 - accuracy: 0.9103 - val_loss: 0.3036 - val_accuracy: 0.8817\n",
      "Epoch 13/27\n",
      "28/28 [==============================] - 11s 380ms/step - loss: 0.2081 - accuracy: 0.9172 - val_loss: 0.2110 - val_accuracy: 0.9194\n",
      "Epoch 14/27\n",
      "28/28 [==============================] - 11s 383ms/step - loss: 0.1768 - accuracy: 0.9322 - val_loss: 0.2140 - val_accuracy: 0.9355\n",
      "Epoch 15/27\n",
      "28/28 [==============================] - 11s 378ms/step - loss: 0.1476 - accuracy: 0.9448 - val_loss: 0.2386 - val_accuracy: 0.8978\n",
      "Epoch 16/27\n",
      "28/28 [==============================] - 10s 375ms/step - loss: 0.1788 - accuracy: 0.9345 - val_loss: 0.2208 - val_accuracy: 0.9086\n",
      "Epoch 17/27\n",
      "28/28 [==============================] - 10s 373ms/step - loss: 0.1693 - accuracy: 0.9345 - val_loss: 0.2822 - val_accuracy: 0.9140\n",
      "Epoch 18/27\n",
      "28/28 [==============================] - 11s 378ms/step - loss: 0.1589 - accuracy: 0.9425 - val_loss: 0.2313 - val_accuracy: 0.9140\n",
      "Epoch 19/27\n",
      "28/28 [==============================] - 11s 381ms/step - loss: 0.1296 - accuracy: 0.9506 - val_loss: 0.2501 - val_accuracy: 0.9140\n",
      "Epoch 20/27\n",
      "28/28 [==============================] - 11s 383ms/step - loss: 0.1345 - accuracy: 0.9494 - val_loss: 0.2937 - val_accuracy: 0.8817\n",
      "Epoch 21/27\n",
      "28/28 [==============================] - 11s 379ms/step - loss: 0.1416 - accuracy: 0.9483 - val_loss: 0.2232 - val_accuracy: 0.9194\n",
      "Epoch 22/27\n",
      "28/28 [==============================] - 11s 379ms/step - loss: 0.1698 - accuracy: 0.9402 - val_loss: 0.1959 - val_accuracy: 0.9247\n",
      "Epoch 23/27\n",
      "28/28 [==============================] - 11s 385ms/step - loss: 0.1301 - accuracy: 0.9517 - val_loss: 0.2636 - val_accuracy: 0.9032\n",
      "Epoch 24/27\n",
      "28/28 [==============================] - 11s 380ms/step - loss: 0.1363 - accuracy: 0.9460 - val_loss: 0.2177 - val_accuracy: 0.9301\n",
      "Epoch 25/27\n",
      "28/28 [==============================] - 11s 377ms/step - loss: 0.1130 - accuracy: 0.9517 - val_loss: 0.2271 - val_accuracy: 0.9140\n",
      "Epoch 26/27\n",
      "28/28 [==============================] - 11s 380ms/step - loss: 0.1016 - accuracy: 0.9621 - val_loss: 0.3292 - val_accuracy: 0.9140\n",
      "Epoch 27/27\n",
      "28/28 [==============================] - 11s 376ms/step - loss: 0.1099 - accuracy: 0.9632 - val_loss: 0.2391 - val_accuracy: 0.9301\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 27\n",
    "history = model.fit(train_padded, Y_train, \n",
    "                    epochs=num_epochs, \n",
    "                    verbose=1, \n",
    "                    shuffle=True,\n",
    "                    validation_data=(val_padded, Y_val),\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../classifiers/trained_models/LSTM_high_acc9437_9140_9157.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 - 0s - loss: 0.3309 - accuracy: 0.9053 - 324ms/epoch - 54ms/step\n",
      "Evaluation loss: 0.33089977502822876\n",
      "Evaluation accuracy: 0.9052631855010986\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_padded, Y_test, verbose=2)\n",
    "print('Evaluation loss:', score[0])\n",
    "print('Evaluation accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
